AI needs Programming Abstractions

In the last year, LLM generated code has become ubiquitous. Satya
Nadella estimates that up to 30% of the code at Microsoft is AI
generated. This development is unsurprising as so much code is often
boilerplate and/or copied from Stack Overflow. Additionally, some of
the most popular coding languages like Python and Java are well-known
for being verbose.

I argue that writing code has long been like doing woodworking with
only hand tools. While crafting small amounts of high-quality code is
reasonable for some use cases (e.g., OS kernel), it also is tedious
and labor-intensive. This property is reflected in the high salaries
for supposedly 10x engineers who are able to produce much more (or
more useful) code than others.

LLM-generated code democratizes the process with pattern matching on a
large corpus of open-source code. Some might think it is a natural
extension of Googling for Stack Overflow code snippets.

How do we leverage LLMs to actually generate code that is useful? Much
like other new technologies, abstraction is the answer. Very few
people write code entirely from scratch today. They use libraries,
operating systems and runtime systems (e.g., memory allocators) and
trust those bits of code to do what they promise.

Programming abstractions is ideal for leveraging LLMs. They are used
to conceal complexity and make it possible to reason about small
pieces of code at a time. I could care less whether the library uses a
high-level programming language or even a lookup table as long as it
fulfills its abstraction contract, does not have bugs and is
efficient.

This attitude is what we should have towards LLM generated code. There
is no way for a person to try and reason about thousands of lines of
code generated by an LLM. But there is also no reason for a person to
reason about their OS kernel as long as it works the way that it
claims to.

Thus, the important parts of the equation are how to ensure that the
LLM generated library code fulfills its model.  Interestingly, that is
a problem that we've long ignored in favor of blindly trusting the
quality of the code or the intelligence of the person writing it. For
example, test-driven development might be a better and how efficient
it is. I could career way to prompt LLMs for code than directly
instructing it to write the code. Black box testing has long been a
way to ensure that the code works the way that you think it does
without inspecting the code itself.

Another option is choosing abstractions that can be verified by
testing or other means. For example, many abstractions concern making
on piece of software or hardware look like another simpler one. For
example, Paxos aims to make replicated machine behave as a single
machine despite failures. Thus, it is possible to verify that the
generated code does behave in that way. 

I argue that it's important to find the right level of programming
abstraction. Plain English is probably too high level and TLA+
specifications are too low level. We definitely need something
declarative, not imperative. 

Of course all of this assumes that the LLMs are actually able to
generate code that works in the first place. One of the serious issues
wiht LLMs right now is that it takes longer to have an LLM write most
reasonable pieces of code than just writing it yourself. But given the
rapidity at which the models are developing, it is probably reasonable
to assume that this will improve.
